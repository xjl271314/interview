---
title: 如何实现大文件上传和断点续传
nav:
  title: 编程题
  path: /writing
  order: 0
group:
  title: 编程题
  path: /writing/project
---

# 如何实现大文件上传和断点续传

- 2022.11.03

## 大文件上传

- 前端

核心是利用 `Blob.prototype.slice` 方法，和数组的 `slice` 方法相似，文件的 `slice` 方法可以返回原文件的某个切片。

预先定义好单个切片大小，将文件切分为一个个切片，然后借助 `http` 的可并发性，同时上传多个切片。这样从原本传一个大文件，变成了并发传多个小的文件切片，可以大大减少上传时间。

另外由于是并发，传输到服务端的顺序可能会发生变化，因此我们还需要给每个切片记录顺序。

- 服务端

服务端负责接受前端传输的切片，并在接收到所有切片后合并所有切片。

这里又引伸出两个问题：

1. 何时合并切片，即切片什么时候传输完成
2. 如何合并切片

第一个问题需要前端配合，前端在每个切片中都携带切片最大数量的信息，当服务端接受到这个数量的切片时自动合并。或者也可以额外发一个请求，主动通知服务端进行切片的合并。

第二个问题，具体如何合并切片呢？这里可以使用 `Nodejs` 的 `读写流（readStream/writeStream）`，将所有切片的流传输到最终文件的流里：

### 前端代码实现

```jsx
import React from 'react';
import { BigFileUpload } from 'interview';

export default () => <BigFileUpload />;
```

### 其他思路

- 计算 hash 耗时的问题，不仅可以通过 `web-workder`，还可以参考 `React`的 `Fiber`架构，通过 `requestIdleCallback`来利用浏览器的空闲时间计算，也不会卡死主线程。
- 文件 `hash`的计算，是为了判断文件是否存在，进而实现`秒传`的功能，所以我们可以参考 `布隆过滤器`的理念, 牺牲一点点的识别率来换取时间，比如我们可以 `抽样算hash`。
- 通过 `web-workder`让 hash 计算不卡顿主线程，但是大文件由于切片过多，过多的 HTTP 链接过去，也会把浏览器打挂 (我试了 4 个 G 的，直接卡死了)， 我们可以通过控制异步请求的 `并发数`来解决。
- 每个切片的上传进度不需要用表格来显示，我们换成方块进度条更直观一些。
- 并发上传中，报错如何重试，比如每个切片我们允许重试两次，三次再终止。
- 由于文件大小不一，我们每个切片的大小设置成固定的也有点略显笨拙，我们可以参考 TCP 协议的 `慢启动策略`， 设置一个初始大小，根据上传任务完成的时候，来动态调整下一个切片的大小， 确保文件切片的大小和当前网速匹配。
- 小的体验优化，比如上传的时候。
- 文件碎片清理。

详细可以参考[https://zhuanlan.zhihu.com/p/104826733]
